# -*- coding: utf-8 -*-
"""Glass_type_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Dyc0RAH7jbpWNLFUarTDn8nTQA8R3VG

# Importing Necessary Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

"""#Loading the dataset"""

data = pd.read_excel('glass(1).xlsx')

"""# Getting the first five rows of dataset"""

data.head()

"""# Getting Info of the dataset

"""

data.info()

"""# Obtaining the statistics of the dataset"""

data.describe()

"""# Checking whether the data has any null values

"""

data.isnull().sum() # The data clearly shows that there are no null values which is a good thing

"""# Performing visualizations to understand the data distribution"""

for col in data.columns:
  plt.figure(figsize=(12, 8))
  feature = data[col]
  sns.histplot(feature,kde = True)

"""# Boxplot

"""

for col in data.columns:
  plt.figure(figsize=(12,6))
  sns.boxplot(data[col],orient='v',palette = 'dark')

"""As random forest is a ensemble learning technique and as the technique uses split-based data separation and  asks series of questions based on the data Random Forestâ€™s internal logic handles many tasks that other algorithms require manual preparation for: scaling, interaction creation, or normalization.RF is Robust to Outliers,No Need for Feature Scaling or Normalization"""

correlation_matrix = data.corr()
print(correlation_matrix)
# Visualizing correlation_matrix using a heatmap
plt.figure(figsize=(10,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

"""# Building the Random forest model"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
features = data.drop('Type',axis = 1)
target = data['Type']
x = features
y = target
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)
rf_model = RandomForestClassifier(n_estimators=100,random_state=42)
rf_model.fit(x_train,y_train)

"""# Checking model performance"""

from sklearn.metrics import accuracy_score,classification_report
y_pred = rf_model.predict(x_test)
print('classification_report:',classification_report(y_test,y_pred))

"""The random forest model has achieved 84% accuracy in predicting the type of glass based on physical and chemical properties.The data is clearly divided into 43 samples and trained.

# Applying Bagging and boosting techniques to observe Accuracy
"""
# Feature importance
feat_importances = pd.Series(rf_model.feature_importances_, index=features.columns)

# Plot top 10 features
plt.figure(figsize=(10,6))
feat_importances.nlargest(10).plot(kind='barh', color='skyblue')
plt.title('Feature Importance in Glass Type Prediction')
plt.xlabel('Importance Score')
plt.gca().invert_yaxis()  # Highest on top
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300)
plt.show()
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
bagging_model = BaggingClassifier(estimator = DecisionTreeClassifier(),n_estimators = 100,random_state =42)
bagging_model.fit(x_train,y_train)
y_pred = bagging_model.predict(x_test)
print('classification_report:',classification_report(y_test,y_pred))

from sklearn.ensemble import AdaBoostClassifier
ada_boost_model = AdaBoostClassifier(n_estimators = 100,random_state = 42)
ada_boost_model.fit(x_train,y_train)
y_pred = ada_boost_model.predict(x_test)
print('classification_report:',classification_report(y_test,y_pred))

from sklearn.ensemble import GradientBoostingClassifier
gb_model = GradientBoostingClassifier(n_estimators = 100,learning_rate = 0.1,random_state = 42)
gb_model.fit(x_train,y_train)
y_pred = gb_model.predict(x_test)
print('classification_report:',classification_report(y_test,y_pred))

# pip install xgboost
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
# Label encoding as xgboost requires target labels that start with 0,1,2,3,4,5,6 instead of 1,2,3,4,5,6,7
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

xgb_model = XGBClassifier(n_estimators =100,learning_rate = 0.1,random_state = 42)
xgb_model.fit(x_train,y_train_enc)
y_pred = xgb_model.predict(x_test)
print('classification_report:',classification_report(y_test_enc,y_pred))

# pip install lightgbm
from lightgbm import LGBMClassifier
lgbm_model = LGBMClassifier(n_estimators =100, random_state =42)
lgbm_model.fit(x_train,y_train)
y_pred = lgbm_model.predict(x_test)
print('classification_report:',classification_report(y_test,y_pred))

#!pip install catboost
from catboost import CatBoostClassifier
cat_model = CatBoostClassifier(iterations = 100,depth = 6,learning_rate = 0.1,random_state = 42)
cat_model.fit(x_train,y_train)
y_pred = cat_model.predict(x_test)
print('classification_report:',classification_report(y_test,y_pred))

"""# Comparing and understanding the accuracy of the different models in predicting the type of the glass

The random forest model has got an accuracy of 84% which is a very good accuracy score.Random forest is a bagging method.
Boosting and bagging are the methods that are said to be increase the accuracy let's have a look whether it's true that bagging and boosting methods truly increase the accuracy in the prediction of glass type. Bagging classifier was first defined then trained with x_train and y_train the model has achieved an accuracy score of 86% that is really a very good score it improved the accuracy upto 2% than the random forest model.There were different types of boosting techniques and i explored all of it in order to achieve a better accuracy. Firstly trained an Ada boost classifier which has achieved an accuracy of 63% which is not a good score the model could have performed better.Gradient boosting classifier has achieved an accuracy of 86% which is the a bit better  accuracy achieved, than  the random forest model.XGBClassifier has got an accuracy of 84% which is equal to the accuracy obtained by the random forest model.The models like LGBMClassifier and CatBoostClassifier also have achieved the equivalent score as random forest.

# Reasons why some of the models have performed well and others did not do so well.

Bagging classifier has an accuracy of 2% increase than the random forest the reason might random forest during data splitting some of the features can be ignoreed to avoid overfitting. whereas in bagging no feature is ignored which could be  the reason of 2% increase of rise in accuracy score.Gradient boosting has also got an accuracy of 86%. the reason might be it completely focuses on correctig the errors of previous models and reduced bias which might be the reason of achieving better accuracy than the random forest.Ada boost has performed very poor maybe because of it is sensitive to outliers.Other models like catboost,LGBMClassifier  and xgboostclassifier has performed well got an accuracy same as Random forest they have failed to capture more accuracy than RF. I would like to conclude that Gradientboostingclassifier and bagging classifier are the best performers interms of obtaining higher accuracy.

# Bagging and Boosting methods

1.Bagging (Bootstrap Aggregating):

Idea: Reduce variance and prevent overfitting by training multiple models independently on random subsets of the data and averaging their predictions.

How it works:

Take multiple bootstrap samples (random samples with replacement) from the original dataset.

Train a base model (like Decision Tree) on each sample.

For prediction:

Classification: Take a majority vote.

Regression: Take the average of all predictions.

Key points:

Each model is independent.

Reduces variance.

Works well with high-variance models like Decision Trees.

Examples: Random Forest is a type of Bagging.
2.Boosting method:

Idea: Reduce bias (and sometimes variance) by training models sequentially, where each new model focuses on correcting errors of previous models.

How it works:

Train a weak model (like a shallow Decision Tree) on the dataset.

Identify which data points were misclassified or had higher error.

Train the next model giving more weight to these difficult points.

Repeat the process sequentially, combining all models for the final prediction.

Key points:

Models are dependent, built sequentially.

Reduces bias and improves accuracy.

Can overfit if too many iterations.

Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM.

# Difference between Bagging and Boosting methods

Bagging builds multiple models independently using random subsets of the data and combines their predictions to reduce variance. It treats all data points equally and is less likely to overfit; Random Forest is a common example. Boosting, on the other hand, builds models sequentially, where each new model focuses on correcting errors made by the previous ones. It gives more weight to difficult data points, reduces bias, but can overfit if not tuned carefully. Examples include AdaBoost, Gradient Boosting, and XGBoost.
"""

